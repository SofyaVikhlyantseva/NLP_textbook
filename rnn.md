# Реккурентные нейронные сети

Люди не начинают думать с нуля каждую секунду. Читая эту главу, вы понимаете каждое слово, отталкиваясь от понимания предыдущих слов, а не отбрасываете всё и начинаете думать заново.
Иными словами, речь человека неслучайна, (статистически) предсказуема; эта идея используется в **языковом моделировании** (*language modeling*) для генерации следующего слова в последовательности.

Один из первых и простейших подходов — **Марковские цепи (цепи Маркова)**, или **N-граммные модели**.

В них делается упрощающее предположение: наличие конкретного слова в тексте объясняется только k словами перед ним.

$p(w_{1}, \ldots, w_{n}) = p(w_{1})p(w_{2}|w_{1})\ldots p(w_{n}|w_{n-1}, \ldots, w_{n-k})$

То есть вероятность совместного появления в тексте слов $w_{1}, \ldots, w_{n}$ равна произведению вероятности появления первого слова на вероятность появления второго слова при условии, что встретилось первое, $\ldots$ на вероятность появления n-ного слова при условии, что встретились k предыдущих.

Пример кода для создания простейшего генератора текста на цепях Маркова:
```python
# подключаем библиотеку numpy
import numpy as np

# Для работы алгоритму всегда нужен исходный текст (корпус) — глядя на него, алгоритм поймёт, какие слова обычно идут друг за другом.
# Здесь возьмём [восьмой том полного собрания сочинений Чехова](https://thecode.media/wp-content/uploads/2021/04/che-1.txt) — повести и рассказы (примерно 150 тыс. слов)
# отправляем в переменную всё содержимое текстового файла
text = open('che.txt', encoding='utf8').read()

# разбиваем текст на отдельные слова (знаки препинания останутся рядом со своими словами)
corpus = text.split()

# с помощью ключевого слова yield делаем новую функцию-генератор, которая определит пары слов
def make_pairs(corpus):
    # перебираем все слова в корпусе, кроме последнего
    for i in range(len(corpus)-1):
        # генерируем новую пару и возвращаем её как результат работы функции
        yield (corpus[i], corpus[i+1])
        
# вызываем генератор и получаем все пары слов
pairs = make_pairs(corpus)

# словарь, на старте пока пустой
word_dict = {}

# перебираем все слова попарно из нашего списка пар
for word_1, word_2 in pairs:
    # если первое слово уже есть в словаре
    if word_1 in word_dict.keys():
        # то добавляем второе слово как возможное продолжение первого
        word_dict[word_1].append(word_2)
    # если же первого слова у нас в словаре не было
    else:
        # создаём новую запись в словаре и указываем второе слово как продолжение первого
        word_dict[word_1] = [word_2]
 
# случайно выбираем первое слово для старта
first_word = np.random.choice(corpus)

# если в нашем первом слове нет больших букв 
while first_word.islower():
    # то выбираем новое слово случайным образом
    # и так до тех пор, пока не найдём слово с большой буквой
    first_word = np.random.choice(corpus)

# делаем наше первое слово первым звеном
chain = [first_word]

# сколько слов будет в готовом тексте
n_words = 100

# делаем цикл с нашим количеством слов
for i in range(n_words):
    # на каждом шаге добавляем следующее слово из словаря, выбирая его случайным образом из доступных вариантов
    chain.append(np.random.choice(word_dict[chain[-1]]))

# выводим результат
print(' '.join(chain))

"""
В октябре 1894 г. Текст статьи, написанные за вечерним чаем сидела за ивы. Они понятия о равнодушии к себе в целом — бич
божий! Егор Семеныч и боялась. В повести пассивности, пессимизма, равнодушия («формализма») писали это она отвечала она
не застав его лоб. Он пишет, что сам Песоцкий впервые явилась мысль о ненормальностях брака. Поймите мои руки; он, — а
женщин небось поставил крест на о. Сахалине (см.: М. — Нет, вы тоже, согласитесь, сытость есть две ночи и белые, пухлые
руки и мог не содержащем единой и не заслуживает «ни закрепления, ни мне не знаю, для меня с 50 рисунками
"""
```

Как видно, результат получился не очень: марковские модели легко реализуемы и работают для коротких шаблонов, но **забывают всё, что было до k слов назад**.

**Рекуррентные нейронные сети** (*Recurrent Neural Networks*, **RNN**) — естественное развитие этой идеи: они стремятся моделировать то, как люди на самом деле читают тексты — "держат в голове" выжимку всего, что уже прочитано.

### Устройство RNN
Рекуррентный = регулярно возвращающийся к чему-то. Нейросеть называется рекуррентной, т.к. в ней есть циклы, позволяющие есть возвращаться к своей работе с прошлого шага и смотреть на информацию в прошлом.

RNN читает текст последовательно слева направо, по одному слову за раз (так же, как люди последовательно читают текст и постепенно понимают, о чём он). На каждом шаге она:
1. получает на вход очередной токен;
2. обновляет **скрытое состояние** — вектор, агрегирующий всю предыдущую информацию;
3. при необходимости — выдаёт предсказание (прогнозы могут быть разными: в задаче языкового моделирования — следующее слово, в pos-tagging — часть речи). Таким образом, это новое состояние содержит информацию о текущем инпуте и информацию со всех предыдущих шагов.

### Формализация
Пусть:
* $x_{t}\in \mathbb{R}^d$ — входной вектор на шаге $t$ (к этому моменту времени прочитали $t$ слов);

$x_{t}$ — закодированное слово: либо one-hot вектор, либо векторное представление (word2vec, fasttext, ...)

* $h_{t}\in \mathbb{R}^H$ — скрытое состояние (например, $H$ = 1000) — вектор, содержащий накопленную информацию после чтения $t$ элементов;

* $W_{xh}\in \mathbb{R}^{H \times d}$, $W_{hh}\in \mathbb{R}^{H \times H}$ — обучаемые матрицы (все элементы матриц — обучаемые параметры, и обучение заключается в нахождении этих параметров).

Тогда:
* $h_{t} = f(W_{xh}x_{t} + W_{hh}h_{t-1})$, где $f$ — функция активации (нелинейность), обычно $\tanh$

То есть, чтобы получить новый скрытый вектор $h_{t}$, складываем входной вектор $x_{t}$, умноженный на матрицу $(W_{xh}$, со скрытым вектором с предыдущего шага $h_{t-1}$, умноженным на матрицу $W_{hh}$, и берём от суммы нелинейность покоординатно.

Это напоминает Momentum (метод инерции) в оптимизации: в нём мы суммируем градиенты с прошлых шагов, немного придавливаем и добавляем новые; здесь мы так же аккуратно накапливаем и сглаживаем информацию.

* если хотим что-то выдавать на каждом шаге, то $o_{t} = f_{o}(W_{ho}h_{t})$, где $f_{o}$ — обычно softmax

То есть берём новый скрытый вектор $h_{t}$, умножаем на матрицу $W_{ho}$, берём от произведения нелинейность и получаем выход — вектор $o_{t}$ (например, вероятности классов).

![RNN](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)
Источник: https://colah.github.io/posts/2015-08-Understanding-LSTMs

*Здесь допишу про  Backpropagation Through Time (BPTT), проблемы обычных RNN (затухающие и взрывающиеся градиенты), LSTM, bidirectional RNN, многослойные RNN, типы RNN (one-to-one, one-to-many, many-to-one, many-to-many)*.

## Рекомендуемые материалы
* [colah's blog: Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

* [Lena Voita. NLP Course | For You. Models: Recurrent (RNN/LSTM/etc)](https://lena-voita.github.io/nlp_course/text_classification.html#main_content)

* [Мама мыла LSTM: как устроены рекуррентные нейросети с долгой краткосрочной памятью](https://sysblok.ru/knowhow/mama-myla-lstm-kak-ustroeny-rekurrentnye-nejroseti-s-dolgoj-kratkosrochnoj-pamjatju/)

* [Lecture 09: Sequences — ИАД, "Основы глубинного обучения"/24, Евгений Андреевич Соколов, НИУ ВШЭ](https://github.com/hse-ds/iad-deep-learning/blob/master/2024/lectures/lecture09-sequences.pdf)

* [CS231n Lecture 8: Recurrent Neural Networks](https://cs231n.stanford.edu/slides/2023/lecture_8.pdf)

* [Daniel Jurafsky & James H. Martin. Speech and Language Processing. Chapter 8: RNNs and LSTMs, 2025](https://web.stanford.edu/~jurafsky/slp3/8.pdf)

* [A. Amidi & Sh. Amidi. CS 230 - Deep Learning: Recurrent Neural Networks cheatsheet](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)
